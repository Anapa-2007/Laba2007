import os
import numpy as np
from PIL import Image

'''Функция для чтения изображений из файла'''
def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        img = img.resize((64, 64))  # Измените размер изображений по вашему усмотрению
        img = np.array(img)
        img = img.astype('float64') / 255
        green_channel = img[:, :, 1]
        even_rows = green_channel[::2, :]
        img = even_rows.flatten()
        images.append(img)
    return images

'''Функция для предотвращения переполнения регрессии'''
def sigmoid(z):
    z[z < -30] = -30
    z[z > 30] = 30
    return 1 / (1 + np.exp(-z))

'''Вычисление стоимости логистической регрессии'''
def compute_cost(X, y, theta):
    m = len(y)
    z = np.dot(X, theta)
    h_x = sigmoid(z)
    J = (-1 / m) * (np.dot(y.T, np.log(h_x)) + np.dot((1 - y).T, np.log(1 - h_x)))
    return J

'''Вычисление градиентов для возврата в градиентную функцию'''
def compute_gradients(X, y, theta):
    m = len(y)
    z = np.dot(X, theta)
    h_x = sigmoid(z)
    gradients = (1 / m) * np.dot(X.T, (h_x - y))
    return gradients

'''Градиентный спуск'''
def gradient_descent(X, y, learning_rate=0.1, num_iterations=1000):
    m, n = X.shape
    theta = np.zeros((n, 1))
    for _ in range(num_iterations):
        grads = compute_gradients(X, y, theta)
        theta -= learning_rate * grads
    return theta

path_to_trees = 'C:/Users/feski/OneDrive/Рабочий стол/Trees/Trees'
path_to_no_trees = 'C:/Users/feski/OneDrive/Рабочий стол/Trees/NoTrees'

trees_images = load_images_from_folder(path_to_trees)
no_trees_images = load_images_from_folder(path_to_no_trees)

# Создание массивов X и y
X = np.vstack((trees_images, no_trees_images))
y = np.hstack((np.ones(len(trees_images)), np.zeros(len(no_trees_images))))

# Перемешивание данных
indices = np.arange(len(X)) # Создание массива с индексами: [0, 1, 2, 3]
np.random.shuffle(indices)
X = np.array([X[i] for i in indices]) # [1, 3, 4, 2]
y = np.array([y[i] for i in indices]) # ['a', 'c', 'd', 'b']
# combined = list(zip(X, y))
# np.random.shuffle(combined)
# X[:], y[:] = zip(*combined)
train_ratio, val_ratio, test_ratio = 0.6, 0.2, 0.2
total_samples = len(X)
train_samples = int(train_ratio * total_samples)
val_samples = int(val_ratio * total_samples)

X_train = X[:train_samples]
y_train = y[:train_samples]

X_val = X[train_samples:train_samples + val_samples]
y_val = y[train_samples:train_samples + val_samples]

X_test = X[train_samples + val_samples:]
y_test = y[train_samples + val_samples:]

# Reshape y for matrix operations
y_train = y_train[:, np.newaxis]
y_val = y_val[:, np.newaxis]
y_test = y_test[:, np.newaxis]

# Add a bias term for theta_0
X_train_ones = np.hstack((np.ones((len(X_train), 1)), X_train))
X_val_ones = np.hstack((np.ones((len(X_val), 1)), X_val))
X_test_ones = np.hstack((np.ones((len(X_test), 1)), X_test))

# Train model
theta = gradient_descent(X_train_ones, y_train)

# Compute prediction
predictions = sigmoid(np.dot(X_test_ones, theta))

# Classify predict as 0 or 1 and compare
true_positive = false_negative = false_positve = 0
mass_true_positive = []
mass_false_negative = []
mass_false_positve = []
for koeff in [i/10 for i in range(0, 11)]:
    for i in range(len(predictions)):
        if predictions[i] >= koeff and y_test[i] == True:
            true_positive += 1
        elif predictions[i] >= koeff and y_test[i] == False:
            false_positve += 1
        elif predictions[i] < koeff and y_test[i] == True:
            false_negative += 1
    mass_true_positive.append(true_positive)
    mass_false_positve.append(false_positve)
    mass_false_negative.append(false_negative)
    true_positive = false_negative = false_positve = 0
# print('', mass_false_negative, mass_false_positve, mass_true_positive, sep='\n')
classified_predictions = (predictions >= 0.5).astype(int)
p, r, f = [], [], []
for i in range(len(mass_true_positive)):
    if mass_false_positve[i] != 0 and mass_true_positive[i] != 0:
        p.append(mass_true_positive[i] / (mass_true_positive[i] + mass_false_positve[i]))
        r.append(mass_true_positive[i] / (mass_true_positive[i] + mass_false_negative[i]))
        f.append(2*p[i]*r[i] / (p[i] + r[i]))
    else:
        f.append(0)
print(f, '\nМаксимум точности и полноты достигается при коэффициенте =', f.index(max(f))/10)

classified_predictions = (predictions >= f.index(max(f))/10).astype(int)
accuracy = (classified_predictions == y_test).mean() * 100
print("% ошибок при 0.9: {:.2f}%".format(100-accuracy))

classified_predictions = (predictions >= 0.5).astype(int)
accuracy = (classified_predictions == y_test).mean() * 100
print("% ошибок при 0.5: {:.2f}%".format(100-accuracy))
print(max(f), '- значение комбинации точности и полноты при оптимальном значении порога')
